"""
Run this script and specify repos to create the persisted file for the web app.

Files generated by this script:
* repositories.txt --- the original repositories file
* invalid_repositories.txt --- the invalid repositories file, including invalid repositories
* filtered_repositories.txt --- the final repositories file, removing duplicated and invalid repositories
* index{i}_{i * target_sub_length}.bin --- the sub-index files, where i means number of sub-repositories and target_sub_length means sub-repositories length
* index.bin --- the index file merged by sub-index files and removed numpy zero arrays
"""

import os
import numpy as np
from docarray import DocList
from docarray.index import InMemoryExactNNIndex
from transformers import pipeline
from tqdm.auto import tqdm
from pathlib import Path
from common.repo_doc import RepoDoc


# REPOS = tuple(
#     input("Input repository names as owner/name, seperated by comma: ").split(",")
# )

def get_model():
    """
    The function for getting the HuggingFace pipeline model
    :return: A HuggingFace pipeline model
    """
    # Option 1 --- Download model by HuggingFace username/model_name
    model_path = "Henry65/RepoSim4Py"

    # Option 2 --- Download model locally
    # model_path = "RepoSim4Py"
    return pipeline(
        model=model_path,
        trust_remote_code=True,
        device_map="auto",
        github_token=os.environ.get("GITHUB_TOKEN")  # Also can be specified by user
    )


def get_repositories():
    """
    The function for getting the original repositories
    :return:
    """
    with open("repositories.txt", "r") as file:
        repositories = file.read().splitlines()
    return repositories


def get_sub_repositories_list(repositories):
    """
    The function for getting sub-repositories list
    :param repositories: the whole repositories
    :return: sub-repositories list and sub-repositories length
    """
    target_sub_length = len(repositories) // 100
    sub_repositories_list = []
    start_index = 0

    while start_index < len(repositories):
        current_length = min(target_sub_length, len(repositories) - start_index)
        current_sub_repositories = repositories[start_index: start_index + current_length]
        sub_repositories_list.append(current_sub_repositories)
        start_index += current_length

    print(f"We totally have {len(sub_repositories_list)} sub-repositories")
    return sub_repositories_list, target_sub_length


def create_index_by_sub(model, sub_repositories_list, target_sub_length):
    """
    The function for using model to generate index by sub-repositories.
    Each index file will be named "index{i}_{i * target_sub_length}.bin", where i means number of sub-repositories
    :param model: the HuggingFace model
    :param sub_repositories_list: the sub-repositories list
    :param target_sub_length: the sub-repositories length
    :return: no return value
    """
    for i, sub_repositories in tqdm(enumerate(sub_repositories_list)):
        tqdm.write(f"Processing sub-repositories {i}")
        dl = DocList[RepoDoc]()
        repo_dataset = model(tuple(sub_repositories))

        for info in repo_dataset:
            dl.append(
                RepoDoc(
                    name=info["name"],
                    topics=info["topics"],
                    stars=info["stars"],
                    license=info["license"],
                    code_embedding=info["mean_code_embedding"].reshape(-1),
                    doc_embedding=info["mean_doc_embedding"].reshape(-1),
                    readme_embedding=info["mean_readme_embedding"].reshape(-1),
                    requirement_embedding=info["mean_requirement_embedding"].reshape(-1),
                    repository_embedding=info["mean_repo_embedding"].reshape(-1)
                )
            )

        index = InMemoryExactNNIndex[RepoDoc](dl)
        index.persist(f"index{i}_{i * target_sub_length}.bin")


def merge_index(target_sub_length):
    """
    The function to merge sub-index files into one whole index file
    :param target_sub_length: the sub-repositories length
    :return: no return value
    """
    INDEX_PATH = Path(__file__).parent.joinpath("index0_0.bin")
    index = InMemoryExactNNIndex[RepoDoc](index_file_path=INDEX_PATH)
    docs = index._docs

    file_name_list = [f"index{i}_{i * target_sub_length}.bin" for i in range(1, 101)]
    for file_name in file_name_list:
        INDEX_PATH_TMP = Path(__file__).parent.joinpath(file_name)
        index_tmp = InMemoryExactNNIndex[RepoDoc](index_file_path=INDEX_PATH_TMP)
        docs_tmp = index_tmp._docs
        docs.extend(docs_tmp)

    index = InMemoryExactNNIndex[RepoDoc](docs)
    index.persist(f"index.bin")


def find_invalid_repositories(file_name):
    """
    The function to find invalid repositories such as username changing or repositories be removed
    :param file_name: the index.bin file
    :return: no return value
    """
    INDEX_PATH = Path(__file__).parent.joinpath(file_name)
    index = InMemoryExactNNIndex[RepoDoc](index_file_path=INDEX_PATH)
    docs = index._docs
    docs_set = set([doc.name for doc in docs])
    repo_set = set(get_repositories())
    diff_set = repo_set - docs_set
    with open("invalid_repositories.txt", "w") as f:
        for repo_name in diff_set:
            print(repo_name, file=f)


def remove_invalid_duplicated_repositories(file_input, file_invalid, file_output):
    """
    The function to remove duplicated and invalid repositories from original repositories file
    :param file_input: the original repositories file
    :param file_invalid: the invalid repositories file
    :param file_output: the filtered repositories file
    :return: no return value
    """
    with open(file_input, "r") as f_input:
        repositories = set(f_input.read().splitlines())
    with open(file_invalid, "r") as f_invalid:
        invalid_repositories = set(f_invalid.read().splitlines())
    with open(file_output, "w") as f_output:
        for repo_name in (repositories - invalid_repositories):
            print(repo_name, file=f_output)


def remove_zero_vectors(file_name):
    """
    The function to remove zero vectors by each specific level
    :param file_name: the index file
    :return: no return value
    """
    INDEX_PATH = Path(__file__).parent.joinpath(file_name)
    index = InMemoryExactNNIndex[RepoDoc](index_file_path=INDEX_PATH)
    docs = index._docs
    for doc in docs:
        if np.all(doc.code_embedding == 0):
            doc.code_embedding = None
        if np.all(doc.doc_embedding == 0):
            doc.doc_embedding = None
        if np.all(doc.requirement_embedding == 0):
            doc.requirement_embedding = None
        if np.all(doc.readme_embedding == 0):
            doc.readme_embedding = None
        if np.all(doc.repository_embedding == 0):
            doc.repository_embedding = None

    index = InMemoryExactNNIndex[RepoDoc](docs)
    index.persist(file_name)


if __name__ == "__main__":
    # Creating sub-index files - 9702//100 pieces in per sub-index file
    model = get_model()
    repositories = get_repositories()
    sub_repositories_list, target_sub_length = get_sub_repositories_list(repositories)
    create_index_by_sub(model, sub_repositories_list, target_sub_length)

    # Merging sub-index files into one whole index file
    merge_index(target_sub_length)

    # Finding invalid repositories
    find_invalid_repositories("index.bin")

    # Removing invalid and duplicated repositories
    remove_invalid_duplicated_repositories("repositories.txt", "invalid_repositories.txt",
                                           "filtered_repositories.txt")

    # Removing numpy zero arrays from index file
    # numpy zero array means that one repository doesn't have specific level such as requirement and readme
    remove_zero_vectors("index.bin")
